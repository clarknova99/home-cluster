---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2beta2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
  namespace: default
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.2.1
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
      interval: 15m
  maxHistory: 3
  install:
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    controllers:
      ollama:
        replicas: 1
        containers:
          app:
            image:
              repository: docker.io/ollama/ollama
              tag: 0.1.44
            env:
              TZ: ${TIMEZONE}
              NVIDIA_VISIBLE_DEVICES: all
              NVIDIA_DRIVER_CAPABILITIES: compute,utility #all
              OLLAMA_HOST: 0.0.0.0
              OLLAMA_ORIGINS: "*"
              OLLAMA_MODELS: /models
              OLLAMA_DEBUG: 1
            resources:
              requests:
                cpu: 2000m
                memory: 4Gi
                #gpu.intel.com/i915: "1"
                nvidia.com/gpu: 1
              limits:
                memory: 12Gi
                nvidia.com/gpu: 1
                #gpu.intel.com/i915: "1"

        pod:
          runtimeClassName: nvidia
          # affinity:
          #   nodeAffinity:
          #     requiredDuringSchedulingIgnoredDuringExecution:
          #       nodeSelectorTerms:
          #         - matchExpressions:
          #             - key: intel.feature.node.kubernetes.io/gpu
          #               operator: In
          #               values:
          #                 - "true"
          nodeSelector:
            #intel.feature.node.kubernetes.io/gpu: "true"
            kubernetes.io/arch: amd64
            kubernetes.io/hostname: k8s-01
            nvidia.feature.node.kubernetes.io/gpu: "true"
        # pod:
        #   nodeSelector:
        #     kubernetes.io/arch: amd64
    service:
      app:
        controller: ollama
        type: LoadBalancer
        annotations:
          io.cilium/lb-ipam-ips: 192.168.2.85
        ports:
          http:
            port: 11434
    ingress:
      app:
        annotations:
          external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
          hajimari.io/enable: "false"
        className: internal
        hosts:
          - host: ollama.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: app
                  port: 11434
        tls:
          - hosts:
              - ollama.${SECRET_DOMAIN}

    persistence:
      config:
        enabled: true
        type: nfs
        server: 192.168.1.2
        path: /volume1/network-storage/ollama/backend
        globalMounts:
          - path: /root/.ollama
      models:
        enabled: true
        type: nfs
        server: 192.168.1.2
        path: /volume1/network-storage/ollama/models
        globalMounts:
          - path: /models
